{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfy5P919JDjo"
      },
      "source": [
        "## **ENGR 2900 Project 2 - Hand Pose Estimation**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "In this Project you will have the opportunity to build a 2D hand pose model and train it on the [Ego-Exo4D dataset](https://ego-exo4d-data.org/) to perform 2D hand pose estimation. The images you will be using are from first-person videos captured by [Project Aria glasses](https://www.projectaria.com/) worn by participants performing various human activities that are highly related to hand movements, e.g. cooking, playing music, bike repairing and covid testing. See Figure 1 for some examples.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <div style=\"display: center; justify-content: space-between;\">\n",
        "        <img src=\"imgs/bike_repairing_sample.png\" alt=\"Bike repairing\" style=\"width: 24%;\">\n",
        "        <img src=\"imgs/cooking_sample.png\" alt=\"Cooking\" style=\"width: 24%;\">\n",
        "        <img src=\"imgs/covid_test_sample.png\" alt=\"Covid test\" style=\"width: 24%;\">\n",
        "        <img src=\"imgs/music_sample.png\" alt=\"Music\" style=\"width: 24%;\">\n",
        "    </div>\n",
        "    <h1 style=\"font-size: 16px;\">Figure 1: Sample images in Ego-Exo4D dataset</h1>\n",
        "</div>\n",
        "\n",
        "**Hand pose estimation**\n",
        "\n",
        "The task of hand pose estimation is usually tackled by a top-down method, meaning a detector is first used to find the hand bounding box (where the hand is at within the original image). Then we get the cropped hand image and feed this into hand pose model as input to perform hand pose estimation. We will follow the top-down method in this project. See Figure 2 about the original image and cropped hand image.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <div style=\"display: center; justify-content: space-between;\">\n",
        "        <img src=\"imgs/top_down_sample.png\" alt=\"top_down\" style=\"width: 80%;\">\n",
        "    </div>\n",
        "    <h1 style=\"font-size: 16px;\">Figure 2: Original image and cropped hand image</h1>\n",
        "</div>\n",
        "\n",
        "**What to do**\n",
        "\n",
        "In this project, your main goal is to implement the second step - to develop and train a 2D hand pose estimation model which predicts the location of each hand joint given a hand image. A list of helper scripts from loading cropped hand image to model training have been provided. The parts that need your work are all within this notebook marked with **TODOS**. See below for Project 2 file structure and helper script introduction.\n",
        "\n",
        "- `imgs/`: this is the directory where sample images for display are stored.\n",
        "- `dataset/`\n",
        "    - `dataset.py`: main Dataset to load and preprocess Ego-Exo4D data for model training.\n",
        "    - `data_util.py`: a list of utility functions to help build the main Dataset, e.g. affine transformation to get cropped hand image.\n",
        "    - `data_vis.py`: some helper functions to visualize the images with hand pose annotation.\n",
        "- `utils/`\n",
        "    - `functions.py`: a list of utility functions to help model training and debugging.\n",
        "    - `loss.py`: implementation of loss function to supervise the model learning.\n",
        "- `ENGR2900_Project2.ipynb`: intergrate every parts from above together from loading dataset to model training and testing. This is where you need to fill in your implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c2-Ca6UJDjs"
      },
      "source": [
        "### 0 - Set-up\n",
        "\n",
        "Run cell below to load in necessary packages and helper scripts. Make sure you have set up and activate the correct environment. If this is your first time runnning this notebook/missing some packages, please refer to README.md for more detail on how to set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "MSgj7oL-JDjs",
        "outputId": "3bd4cae7-7ef4-4e4b-c247-ed539c43a09b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.dataset import ego4dDataset\n",
        "from dataset.data_vis import *\n",
        "from utils.loss import Heatmap_Loss\n",
        "import torchvision.transforms as transforms\n",
        "from utils.function import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFT9wQdIJDjt"
      },
      "source": [
        "### 1 - Load Ego4D dataset\n",
        "\n",
        "To start with, let's load in the Ego-Exo4D data and visualize the ground truth pose annotation. The data we will be using is a subset of the large original Ego-Exo4D dataset, and you can download the annotation JSON files and images from [here](https://drive.google.com/drive/folders/1IWfQIfKzwvSfcBf9G2V18edkkk6SqE6M?usp=sharing). Put the annotation under `anno_dir` and unzipped images under `img_dir` wtih file structure looks like this:\n",
        "\n",
        "\n",
        "```\n",
        "{anno_dir}\n",
        "    ├── ego_pose_gt_anno_test.json\n",
        "    ├── ego_pose_gt_anno_train.json\n",
        "    └── ego_pose_gt_anno_val.json\n",
        "```\n",
        "\n",
        "```\n",
        "{img_dir}\n",
        "    ├── train\n",
        "    │   ├── cmu_bike01_1\n",
        "    │   ├── ...\n",
        "    │\n",
        "    ├── val\n",
        "    │   ├── georgiatech_bike_07_10\n",
        "    │   ├── ...\n",
        "    │\n",
        "    └── test\n",
        "        ├── cmu_bike06_2\n",
        "        ├── ...\n",
        "```\n",
        "\n",
        "*NOTE: Ego-Exo4D dataset annotation is still in progress, thus there might be some bad/missing annotations. However, the general quality of the dataset should be good enough for you to train a hand pose estimation model that gives reasonable predictions.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7KQ7KCjJDju"
      },
      "outputs": [],
      "source": [
        "# TODO: Modify config as needed, e.g. annotation and image directory, training batch size etc.\n",
        "cfg = {\n",
        "        \"anno_dir\": ...,\n",
        "        \"img_dir\": ...,\n",
        "        \"lr\": 1e-1,\n",
        "        \"train_bs\": 64,\n",
        "        \"val_bs\": 64,\n",
        "        \"epochs\": 20,\n",
        "        \"image_dim\": 112,\n",
        "        \"heatmap_dim\": 112,\n",
        "        \"img_mean\": [0.485, 0.456, 0.406],\n",
        "        \"img_std\": [0.229, 0.224, 0.225],\n",
        "    }\n",
        "\n",
        "# Define the transform for image data preprocessing, which in here is just image normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cfg[\"img_mean\"],\n",
        "                         std=cfg[\"img_std\"])\n",
        "])\n",
        "\n",
        "# TODO: Initialize the train, val and test Dataset\n",
        "# Hint: take a look at the implementation of ego4dDataset for initialization requirement\n",
        "train_dataset = ego4dDataset(cfg=...,\n",
        "                             split=...,\n",
        "                             transform=...)\n",
        "\n",
        "val_dataset = ego4dDataset(cfg=...,\n",
        "                           split=...,\n",
        "                           transform=...)\n",
        "\n",
        "test_dataset = ego4dDataset(cfg=...,\n",
        "                            split=...,\n",
        "                            transform=...)\n",
        "\n",
        "# Check the dataset length\n",
        "print(\"Train: \", len(train_dataset))\n",
        "print(\"Val: \", len(val_dataset))\n",
        "print(\"Test: \", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfB5d_SLJDju"
      },
      "source": [
        "**Visualizing the dataset**\n",
        "\n",
        "With the top-down method, the hand pose estimation task is further defined as a heatmap regression task. Instead of directly predicting the 2D (x,y) coordinates of each hand joints, we represent each 21 hand joints (See Figure 3 for joint index relation) as a heatmap s.t. a higher value means a higher probabilities of a ground truth joint. The goal of the model is to regress each joint position by producing a set of heatmaps, and we can get the predicted (x,y) coordinate of each joint by selecting the point with maximum values in heatmap as shown in Figure 4.\n",
        "\n",
        "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
        "    <figure>\n",
        "        <img src=\"imgs/hand_index.png\" alt=\"Hand index\" width =\"300\" height=\"300\">\n",
        "        <figcaption>Figure 3: Hand index</figcaption>\n",
        "    </figure>\n",
        "    <figure>\n",
        "        <img src=\"imgs/hand_pose_hm.png\" alt=\"Hand pose heatmap\" width =\"900\" height=\"280\">\n",
        "        <figcaption>Figure 4: Hand pose estimation from heatmap</figcaption>\n",
        "    </figure>\n",
        "</div>\n",
        "\n",
        "In cell below, you can select data from a different dataset and visualize those ground truth poses to get a better understanding about the heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLNRYtslJDju"
      },
      "outputs": [],
      "source": [
        "## TODO: Modify as needed to take a look at the dataset\n",
        "check_dataset = train_dataset\n",
        "idx = ...\n",
        "\n",
        "## Get one dataset sample for visualization\n",
        "input, gt_heatmap, gt_kpts_weight, metadata = check_dataset[idx]\n",
        "\n",
        "## Visualization based on heatmap\n",
        "# 1. Transform normalized image back to original RGB images in [0,1]\n",
        "vis_img = inverse_normalize(input, cfg[\"img_mean\"], cfg[\"img_std\"]).permute(1,2,0)\n",
        "# 2. Find (x,y) of each joint from ground truth heatmap\n",
        "gt_kpts_from_hm = get_max_preds(gt_heatmap.unsqueeze(0).numpy()).squeeze(0)\n",
        "# 3. Remove invalid joints by assigned None\n",
        "gt_kpts_from_hm[~gt_kpts_weight] = None\n",
        "# Visualization\n",
        "vis_data(vis_img, gt_kpts_from_hm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEAeIsTsJDjv"
      },
      "source": [
        "### 2 - Define model\n",
        "\n",
        "The model we will be using is a modified version of [U-Net](https://arxiv.org/abs/1505.04597) with fewer layers for faster training with reasonable performance. Assume batch_size=1, it takes a RGB image $I (1,3,112,112)$ as input and produce a set of heatmaps $H (1,21,112,112)$ as output. See Figure 5 for model architecture details, where each box corresponds to a multi-channel feature map whose number of channels is denoted on top and spatial dimensions is denoted at the bottom left. If no numbers denoted, then it has the same value as previous one. Each arrow denotes some operations.\n",
        "\n",
        "<div style=\"display: center; justify-content: space-between; text-align: center;\">\n",
        "    <figure>\n",
        "        <img src=\"imgs/model.png\" alt=\"Model architecture\" style=\"width: 80%;\">\n",
        "        <figcaption>Figure 5: Model architecture</figcaption>\n",
        "    </figure>\n",
        "</div>\n",
        "\n",
        "The model architecture shapes like the English letter U which is why it is called U-Net. `D` is a hyperparameter we could control over the number of expanded channels, which is 16 by default. The feature map first follows a contracting path, where the spatial dimension is reduced x2 by pooling and number of channels is increased by convolutions. Then it follows an expansive path, where the spatial dimension is increased x2 by upsampling and number of channels is reduced by convolutions. The feature maps from earlier layers are also concatenated with feature maps in later layers in expansive path so that the features from eariler layers are reused, which proves to help model convergence and generate better result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_1IVYM8JDjv"
      },
      "source": [
        "Also notice the repetitive existence of two consecutive blue arrows in the model architecture, which represent two 3x3 convolutions with batch normalization and ReLU operation. They only change the channels of the feature map but keep spatial dimension unchanged. It turns out we can include those operations denoted by these two consecutive blue arrows as a separate module `ConvBlock`, which only takes two parameters `in_ch`and `out_ch` as input to define all the inner layers, with more details about each layer from table below. In this way, we can greatly simplify our model module design by reusing the `ConvBlock` module to avoid repetition.\n",
        "\n",
        "**ConvBlock:**\n",
        "|  Operation  |                                Hyperparameters                                |\n",
        "|:-----------:|:-----------------------------------------------------------------------------:|\n",
        "|    Conv2d   |  in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1, bias=False |\n",
        "| BatchNorm2d |                              num_features=out_ch                              |\n",
        "|     ReLU    |                                                                               |\n",
        "|    Conv2d   | in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1, bias=False |\n",
        "| BatchNorm2d |                              num_features=out_ch                              |\n",
        "|     ReLU    |                                                                               |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jArdswSiJDjv"
      },
      "source": [
        "All the pooling/upsampling operation can be performed with the same `torch.nn` module (since there is no trainable weights), see below for their hyperparameters:\n",
        "\n",
        "| Operation |                    Hyperparameters                   |\n",
        "|:---------:|:----------------------------------------------------:|\n",
        "| MaxPool2d |                     kernel_size=2                    |\n",
        "|  Upsample | scale_factor=2, mode=\"bilinear\", align_corners=False |\n",
        "\n",
        "The final convolution operations (represented by the pink arrow) is just one 3x3 convolution with sigmoid function:\n",
        "\n",
        "| Operation |                            Hyperparameters                            |\n",
        "|:---------:|:---------------------------------------------------------------------:|\n",
        "|   Conv2d  | in_channels=D, out_channels=21, kernel_size=3, padding=1, bias=False  |\n",
        "|  Sigmoid  |                                                                       |\n",
        "\n",
        "With all those information, please fill in the TODOs below to build the entire model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMHOatDiJDjv"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # TODO: Define each layer as shown above\n",
        "        # Hint: You can use nn.Sequential() to organize all operations together.\n",
        "        # See https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#sequential\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Pass input through each layers and return final output\n",
        "        return ...\n",
        "\n",
        "\n",
        "class ShallowUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture of shallower UNet for 2D hand pose estimation\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channel=3, out_channel=21, MODEL_NEURONS=16):\n",
        "        super().__init__()\n",
        "        # TODO: Define each layers and operations in model\n",
        "        # Hint: reuse ConvBlock\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Pass input through each layers/operations and return final output\n",
        "        # Hint: to concat two tensors you can use torch.concat()\n",
        "        return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRXgFt9JDjw"
      },
      "outputs": [],
      "source": [
        "# Run this test cell to check the model output shape\n",
        "dummy_input = torch.rand(1,3,112,112)\n",
        "model = ShallowUNet()\n",
        "dummy_output = model(dummy_input)\n",
        "assert dummy_output.shape == (1,21,112,112), \"Incorrect model output shape. Please check your model design\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCD0lHDLJDjw"
      },
      "source": [
        "### 3 - Train model\n",
        "\n",
        "In this part, you need to define the pipeline of model training and validation similar to Project 1. See below TODOs to fill in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikH8BKouJDjw"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, criterion, optimizer, device, output_dir):\n",
        "    # TODO: set model to training mode\n",
        "\n",
        "\n",
        "    # Keep track of training loss\n",
        "    batch_loss = []\n",
        "    debug_step = len(train_loader) // 6\n",
        "\n",
        "    train_loader = tqdm(train_loader, dynamic_ncols=True)\n",
        "    # Iterate over all training samples\n",
        "    for i, (input, gt_hm, kpts_weight, _) in enumerate(train_loader):\n",
        "        # TODO:\n",
        "        # 1. Put all revelant data onto same device\n",
        "        # 2. Model forward (given cropped hand image, predict a set of heatmaps)\n",
        "        pred_hm = ...\n",
        "\n",
        "\n",
        "        # TODO: Compute loss\n",
        "        loss = ...\n",
        "        batch_loss.append(loss.item())\n",
        "\n",
        "\n",
        "        # TODO:\n",
        "        # 1. Clear the old parameter gradients\n",
        "        # 2. Compute the derivative of loss w.r.t the model parameters\n",
        "        # 3. Update the model parameters with optimizer\n",
        "\n",
        "\n",
        "        # Save debugging images every debug_step\n",
        "        if (i+1) % debug_step == 0:\n",
        "            # Save debugging images\n",
        "            prefix = '{}_{}'.format(os.path.join(output_dir, 'train'), i)\n",
        "            pred_kpts = get_max_preds(pred_hm.detach().cpu().numpy())\n",
        "            gt_kpts = get_max_preds(gt_hm.detach().cpu().numpy())\n",
        "            save_debug_images(input, gt_kpts, gt_hm, pred_kpts, pred_hm, kpts_weight, prefix)\n",
        "    # Return average training loss\n",
        "    return np.mean(batch_loss)\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, device, output_dir):\n",
        "    # TODO: set model to evaluate mode\n",
        "\n",
        "\n",
        "    # Keep track of validation loss\n",
        "    batch_loss = []\n",
        "    debug_step = len(val_loader) // 6\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_loader = tqdm(val_loader, dynamic_ncols=True)\n",
        "        # Iterate over all validation samples\n",
        "        for i, (input, gt_hm, kpts_weight, _) in enumerate(val_loader):\n",
        "            # TODO:\n",
        "            # 1. Put all revelant data onto same device\n",
        "            # 2. Model forward (given cropped hand image, predict a set of heatmaps)\n",
        "            pred_hm = ...\n",
        "\n",
        "\n",
        "            # Compute loss\n",
        "            loss = ...\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "\n",
        "            # Save debugging images every debug_step\n",
        "            if (i+1) % debug_step == 0:\n",
        "                # Save debugging images\n",
        "                prefix = '{}_{}'.format(os.path.join(output_dir, 'val'), i)\n",
        "                # Get pred and gt kpts and scale back to image scale\n",
        "                pred_kpts = get_max_preds(pred_hm.detach().cpu().numpy())\n",
        "                gt_kpts = get_max_preds(gt_hm.detach().cpu().numpy())\n",
        "                save_debug_images(input, gt_kpts, gt_hm, pred_kpts, pred_hm, kpts_weight, prefix)\n",
        "    # Return average validation loss\n",
        "    return np.mean(batch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UEuI-ZhJDjw"
      },
      "source": [
        "With the training and validation pipeline set up, we can then instantiate our model and define other essential parts for model training including:\n",
        "\n",
        "- **optimizer:** We will be using stochastic gradient descent as our optimizer, with initial learning rate set at `cfg[\"lr\"]` which is 0.1 by default.\n",
        "\n",
        "- **loss function (criterion):** We will use `Heatmap_Loss()` as our loss function (the implementation is already provided), which is built based on Dice loss to compute the difference between predicted and ground truth heatmap across each joint to regulate the model learning. See this [post](https://pycad.co/the-difference-between-dice-and-dice-loss/) for visual illustration on how Dice loss works. Please take a look at the `Heatmap_Loss()` implementation to see how to properly call it. Remember to pass `gt_kpts_weight` as third argument to loss function so that for those invalid joints they won't be included in loss calculation, thus doesn't affect the training process.\n",
        "\n",
        "- **scheduler:** scheduler helps to adjust the learning rate to ensure smoother and better training process. To start with, use `MultiStepLR` scheduler to decay the learning rate by 0.1x at epoch 6 and 12. Feel free to try with [different scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) as you needed.\n",
        "\n",
        "- **dataloaders**: train and val dataloaders to return data in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AzhEh-fJDjx"
      },
      "outputs": [],
      "source": [
        "# TODO: Instantiate the model and define device for training to use GPU if available\n",
        "device = ...\n",
        "model = ...\n",
        "\n",
        "# TODO: Define criterion\n",
        "criterion = ...\n",
        "# TODO: Define optimizer\n",
        "optimizer = ...\n",
        "# TODO: Define scheduler\n",
        "scheduler = ...\n",
        "\n",
        "# Define train and val dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=cfg[\"train_bs\"],\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=cfg[\"val_bs\"],\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2OEymzmJDjx"
      },
      "outputs": [],
      "source": [
        "# Define output directory (where debugging images and model ckpt will be saved)\n",
        "output_root = \"output\"\n",
        "output_dir = os.path.join(output_root, time.strftime(\"%Y-%m-%d-%H-%M\"))\n",
        "print(\"=\"*10 + f\" Training started. Output will be saved at {output_dir} \" + \"=\"*10)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Keep track of each epoch train and val loss\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "\n",
        "# Start training\n",
        "best_val_loss = np.inf\n",
        "for e in range(cfg[\"epochs\"]):\n",
        "    # Train\n",
        "    epoch_train_loss = train(train_loader, model, criterion, optimizer, device, output_dir)\n",
        "    all_train_loss.append(epoch_train_loss)\n",
        "\n",
        "    # Val\n",
        "    epoch_val_loss = validate(val_loader, model, criterion, device, output_dir)\n",
        "    all_val_loss.append(epoch_val_loss)\n",
        "\n",
        "    # Scheduler step\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "    # Log loss\n",
        "    print(\"Epoch: {}/{} \\t Train loss: {:.5f} \\t Val loss: {:.5f}\".format(e, cfg[\"epochs\"], epoch_train_loss, epoch_val_loss))\n",
        "\n",
        "    # Check for best validation loss\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        # Save best model weight with smallest validation loss\n",
        "        torch.save(model.state_dict(), os.path.join(output_dir, f\"final_state.pth.tar\"))\n",
        "        print(f\"Saving model weight with best val_loss={epoch_val_loss:.5f}\")\n",
        "    print()\n",
        "print(\"=\"*10 + f\" Training finished. Got best model with val_loss={best_val_loss:.5f} \" + \"=\"*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk-MX3m1JDjx"
      },
      "source": [
        "### 4 - Inference\n",
        "\n",
        "In this part we will load in the pretrained model weight you got from previous part, and do the model inference on test set to evaluate its performance. By visualizing the predicted hand pose and ground truth pose, we can get an idea whether the model actually learns to find the location of each hand joint from an image.\n",
        "\n",
        "Considering the fact that the model is fairly simple and there exists some noisy data samples in the dataset, you are not expected to get a superior model gives excellent predictions for all images. However, for images with clear hand present e.g. hands playing pianos, the model should give reasonably good predictions in general. See one example below on how the model actually predicts a better hand pose compared with the noisy ground truth annotation on one test image.\n",
        "\n",
        "<div style=\"display: center; justify-content: space-between; text-align: center;\">\n",
        "    <img src=\"imgs/model_inference_sample.png\" alt=\"Example of better model prediction than noisy ground truth\" width=\"700\" height=\"360\">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz_-yJPoJDjx"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize model and load in pretrained weight\n",
        "# Hint: use given helper function load_pretrained_weights() to load in model weight\n",
        "model = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4zsjd6hJDjx"
      },
      "outputs": [],
      "source": [
        "# TODO: Modify index to check model inference result on different images\n",
        "inference_dataset = test_dataset\n",
        "idx = ...\n",
        "\n",
        "# Get one data sample from test set\n",
        "input, gt_hm, kpts_weight, meta = inference_dataset[idx]\n",
        "vis_img = inverse_normalize(input, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).permute(1,2,0)\n",
        "\n",
        "# GT\n",
        "gt_kpts = get_max_preds(gt_hm.unsqueeze(0).detach().cpu().numpy())\n",
        "gt_kpts = gt_kpts.squeeze(0)\n",
        "gt_kpts[~kpts_weight] = None\n",
        "vis_data(vis_img, gt_kpts, \"gt\")\n",
        "\n",
        "# Prediction\n",
        "with torch.no_grad():\n",
        "    pred_hm = model(input.unsqueeze(0))\n",
        "pred_kpts = get_max_preds(pred_hm.detach().cpu().numpy())\n",
        "pred_kpts = pred_kpts.squeeze(0)\n",
        "pred_kpts[~kpts_weight] = None\n",
        "vis_data(vis_img, pred_kpts, \"pred\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNm7Pr7BKOzB"
      },
      "source": [
        "### 5 - Reflection\n",
        "\n",
        "TODO: Discuss as a group and answer the following questions in an new cell below.\n",
        "1. To what challenges could a solution be developed to address, that incorporates hand pose estimation? You may disucss more than one idea.\n",
        "2. To what challenges could a solution be developed to address, that incorporates body pose estimation? You may disucss more than one idea.\n",
        "3. If any, which part(s) of this assignment did you find most challenging to understand and/or implement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03P38EymJDjy"
      },
      "source": [
        "### 6 - Submission\n",
        "\n",
        "TODO: After you have completed this notebook, submit the following items listed below to Gradescope. Only one group member needs to make a submission, and add the other group member.\n",
        "\n",
        "Requirements for submission:\n",
        "1. The completed .ipynb file in .ipynb format (you must run all cells before saving the file for submission).\n",
        "2. The completed .ipynb file in .pdf format (you must run all cells before saving the file for submission).\n",
        "3. 3 images from the test dataset visualizing hand-pose estimation after running inference."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "potter_pose",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
